{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# advanced autcorrect and autocomplete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "# dataset = load_dataset(\"Skylion007/openwebtext\",split='train[:1%]')\n",
    "dataset = load_dataset(\"bookcorpus\",split='train[:50%]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=dataset.to_pandas()\n",
    "ds=ds['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)\n",
    "print(type(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk   \n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt      \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(ds[0]))\n",
    "print('number of the train dataset :',len(ds))\n",
    "print('\\033[92m' + ds[random.randint(0,5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                  \n",
    "import string                              \n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the stopwords from NLTK\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the english stop words list from NLTK\n",
    "stopwords= stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(text_example):\n",
    "    \n",
    "    # convert all letters to lower case\n",
    "    example = text_example.lower()\n",
    "\n",
    "    # Remove links\n",
    "    example = re.sub(r'http\\S+|www.\\S+|@|️#|', '', example)\n",
    "\n",
    "    # Remove other non-alphanumeric characters \n",
    "    example = re.sub(r'[^a-zA-Z0-9 .]', ' ', example)\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    sentence_tokens = word_tokenize(example)\n",
    "    \n",
    "    sentence_tokens = [token for token in sentence_tokens if token.strip() and token not in stopwords and token !='#']\n",
    "        \n",
    "    sentence_tokens = [token for token in sentence_tokens if token and token != '️']\n",
    "    \n",
    "    sentence_tokens =['<s>']+sentence_tokens +['</s>']\n",
    "    \n",
    "    return sentence_tokens\n",
    "\n",
    "\n",
    "# def text_processing(text_example):\n",
    "#     # convert all letters to lower case\n",
    "#     example = text_example.lower()\n",
    "\n",
    "#     # Remove links\n",
    "#     example = re.sub(r'http\\S+|www.\\S+|@|️#|', '', example)\n",
    "    \n",
    "\n",
    "#     # Split the text into sentences\n",
    "#     sentences = sent_tokenize(example)\n",
    "    \n",
    "    \n",
    "#     processed_text = []\n",
    "#     for sentence in sentences:\n",
    "        \n",
    "\n",
    "    \n",
    "#         # Tokenize the sentence\n",
    "#         sentence_tokens = nltk.word_tokenize(sentence)\n",
    "        \n",
    "            \n",
    "#         sentence_tokens = [token for token in sentence_tokens if token.strip() and token not in stopwords and token !='#']\n",
    "        \n",
    "#         sentence_tokens = [token for token in sentence_tokens if token and token != '️']\n",
    "        \n",
    "#         sentence_tokens =['<s>']+sentence_tokens +['</s>']\n",
    "\n",
    "#         processed_text.extend(sentence_tokens)\n",
    "    \n",
    "#     return processed_text\n",
    "\n",
    "print('original text: ',ds[0])\n",
    "print('processing text : ',text_processing(ds[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=ds[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=ds.apply(text_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_token(token, tokenized_sentences):\n",
    "    for sentence in tokenized_sentences:\n",
    "        if token in sentence:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "token = '#'\n",
    "exists = check_token(token, ds)\n",
    "\n",
    "print(f\"Does the token '{token}' exist in the tokenized sentences? {exists}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(ds)\n",
    "total_words = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "all_words = [word for sentence in ds for word in sentence]\n",
    "print(len(Counter(all_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokenizer.word_index))\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(sentences):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(1, len(sentence)):\n",
    "            inputs.append(sentence[:i])\n",
    "            targets.append(sentence[i])\n",
    "            \n",
    "    return inputs, targets\n",
    "\n",
    "inputs, targets = generate_sequences(ds)\n",
    "print(\"Inputs:\", inputs[:2])\n",
    "print(\"Targets:\", targets[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "def plot_sequence_lengths(sequences):\n",
    "    # Calculate the lengths of the sequences\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "    # Count the frequencies of each length\n",
    "    counter = Counter(lengths)\n",
    "\n",
    "    # Separate the lengths and their frequencies\n",
    "    lengths = list(counter.keys())\n",
    "    frequencies = list(counter.values())\n",
    "\n",
    "    # Plot the frequencies of each length\n",
    "    plt.bar(lengths, frequencies)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Frequencies of Sequence Lengths')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Plot the frequencies of the sequence lengths\n",
    "plot_sequence_lengths(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_optimal_maxlen(sequences):\n",
    "    # Calculate the lengths of the sequences\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "    # Calculate the mean and standard deviation of the lengths\n",
    "    mean = np.mean(lengths)\n",
    "    std = np.std(lengths)\n",
    "\n",
    "    # Set the optimal max length to be the mean plus one standard deviation\n",
    "    optimal_maxlen = int(mean + std)\n",
    "\n",
    "    return optimal_maxlen\n",
    "\n",
    "\n",
    "# Find the optimal max length for padding\n",
    "optimal_maxlen = find_optimal_maxlen(inputs)\n",
    "\n",
    "print(f\"The optimal max length for padding is {optimal_maxlen}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def generate_tokenized_sequences(sentences,total_words):\n",
    "#     # Initialize the tokenizer\n",
    "#     tokenizer = Tokenizer()\n",
    "    \n",
    "#     # Flatten the list of sentences and fit the tokenizer\n",
    "#     all_words = [word for sentence in sentences for word in sentence]\n",
    "#     tokenizer.fit_on_texts(all_words)\n",
    "    \n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(1, len(sentence)):\n",
    "            # Convert the sentence up to i+1 to a sequence\n",
    "            sequence = tokenizer.texts_to_sequences([sentence[:i+1]])[0]\n",
    "            sequences.append(sequence)\n",
    "    \n",
    "    # Pad the sequences\n",
    "    sequences = pad_sequences(sequences)\n",
    "    \n",
    "    # Split the padded sequences into inputs and targets\n",
    "    inputs = sequences[:, :-1]\n",
    "    targets = sequences[:, -1]\n",
    "    targets = to_categorical(targets, num_classes=total_words)\n",
    "\n",
    "    \n",
    "    return inputs, targets #,tokenizer\n",
    "\n",
    "inputs, targets = generate_tokenized_sequences(ds,total_words)\n",
    "\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Targets:\", targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(targets[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sequences_length(sequences):\n",
    "    # Get the length of the first sequence\n",
    "    first_sequence_length = len(sequences[0])\n",
    "\n",
    "    # Check if all sequences have the same length\n",
    "    return all(len(sequence) == first_sequence_length for sequence in sequences)\n",
    "\n",
    "print(check_sequences_length(inputs))  # Outputs: True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size =len(tokenizer.word_index)+1\n",
    "maxlen=len(inputs[0])\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense\n",
    "\n",
    "# vocab_size, embedding_dim = embedding_matrix.shape\n",
    "# print(vocab_size, embedding_dim)\n",
    "model = Sequential([\n",
    "#     Embedding(input_dim=vocab_size, output_dim=embedding_dim, trainable=False),\n",
    "    Embedding(total_words, 300),\n",
    "    Bidirectional(LSTM(units=128, return_sequences=True, dropout=0.2)),\n",
    "    Bidirectional(LSTM(units=128, dropout=0.2)),\n",
    "    Dense(units=128, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "    Dense(units=vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# model.build(input_shape=(None, maxlen))\n",
    "\n",
    "# # Now set the weights of the Embedding layer\n",
    "# model.layers[0].set_weights([embedding_matrix])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300\n",
    "history = model.fit(inputs, targets,batch_size=batch_size,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(inputs, targets, batch_size):\n",
    "    num_samples = len(inputs)\n",
    "    while True:  # Loop forever so the generator never terminates\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_inputs = inputs[i:i+batch_size]\n",
    "            batch_targets = targets[i:i+batch_size]\n",
    "            yield batch_inputs, batch_targets\n",
    "\n",
    "batch_size = 2860\n",
    "train_generator = data_generator(inputs, targets, batch_size)\n",
    "\n",
    "history = model.fit(train_generator, steps_per_epoch=len(inputs)//batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
